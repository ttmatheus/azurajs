---
title: Cluster Mode
description: Scale your application across multiple CPU cores automatically for maximum performance
icon: Monitor
---

# Cluster Mode ğŸ–¥ï¸

AzuraJS provides **built-in cluster mode support** to automatically scale your application across all available CPU cores with **zero manual configuration**. Cluster mode dramatically improves throughput and reliability by running multiple instances of your application simultaneously.

<Callout type="info">
**What is Cluster Mode?** Cluster mode creates multiple worker processes (one per CPU core) that all handle incoming requests. The OS load balancer distributes traffic evenly across workers, maximizing CPU utilization.
</Callout>

## Why Use Cluster Mode? ğŸ¯

Cluster mode provides significant benefits for production applications:

### Performance Benefits ğŸ“ˆ
- **Higher throughput**: Handle 3-15x more requests per second
- **Better CPU utilization**: Use all available cores instead of just one
- **Reduced latency**: Distribute load across multiple processes
- **Concurrent processing**: Handle multiple heavy operations simultaneously

### Reliability Benefits ğŸ›¡ï¸
- **Automatic recovery**: Crashed workers are automatically restarted
- **Zero downtime**: Other workers continue serving while one restarts
- **Process isolation**: Issues in one worker don't affect others
- **Graceful shutdown**: Workers finish current requests before stopping

### When to Enable âœ…
- âœ… **Production environments** with high traffic
- âœ… **Multi-core servers** (2+ CPU cores)
- âœ… **CPU-intensive operations** (data processing, image manipulation)
- âœ… **High availability requirements**
- âœ… **Long-running processes** that need resilience

### When NOT to Enable âŒ
- âŒ **Development/debugging** (single process is easier to debug)
- âŒ **Single-core machines** (no benefit, adds overhead)
- âŒ **Container orchestration** (Kubernetes/Docker Swarm handle scaling)
- âŒ **Serverless functions** (already auto-scaled)
- âŒ **Stateful applications** without external session storage

## Quick Start âš¡

### Basic Setup

Simply enable cluster mode in your configuration file and AzuraJS handles everything automatically:

```typescript title="azura.config.ts"
import type { ConfigTypes } from "azurajs/config";

const config: ConfigTypes = {
  server: {
    port: 3000,
    cluster: true,  // Enable cluster mode - that's it!
  },
};

export default config;
```

That's it! When `cluster: true` is set, AzuraJS automatically:
- âœ… Detects the number of CPU cores available
- âœ… Spawns one worker process per CPU core
- âœ… Distributes incoming connections across workers using round-robin
- âœ… Automatically restarts crashed workers with exponential backoff
- âœ… Handles graceful shutdown of all workers
- âœ… Manages inter-process communication
- âœ… Synchronizes configuration across all workers

### Your Application Code Stays Simple

No changes needed to your application code! Your routes and controllers work exactly the same:

```typescript title="index.ts"
import { AzuraClient } from "azurajs";
import { applyDecorators } from "azurajs/decorators";
import { HomeController } from "./controllers/HomeController";

const app = new AzuraClient();

applyDecorators(app, [HomeController]);

// This automatically creates workers when cluster mode is enabled
await app.listen();
```

<Callout type="success">
**Zero Code Changes**: Your application code remains identical whether cluster mode is enabled or not. All clustering logic is handled internally by AzuraJS.
</Callout>



## How It Works ğŸ”§

AzuraJS implements the Node.js cluster module internally with automatic worker management and intelligent load balancing.

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Primary Process                 â”‚
â”‚  - Spawns workers                       â”‚
â”‚  - Monitors health                      â”‚
â”‚  - Manages restarts                     â”‚
â”‚  - Handles signals                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚          â”‚          â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Worker 1 â”‚ â”‚Worker 2â”‚ â”‚Worker Nâ”‚
    â”‚ (Port 3k)â”‚ â”‚(Port 3k)â”‚ â”‚(Port 3k)â”‚
    â””â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”˜ â””â”€â”€â–²â”€â”€â”€â”€â”€â”˜ â””â”€â”€â–²â”€â”€â”€â”€â”€â”˜
          â”‚         â”‚          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚    OS Load Balancer             â”‚
    â”‚    (Round-Robin)                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
            Client Requests
```

### Internal Process Flow

1. **Primary Process Creation**
   - AzuraJS creates a primary process on startup
   - Primary doesn't handle requests, only manages workers

2. **Worker Spawning**
   - One worker per CPU core is forked
   - Each worker runs a complete copy of your application
   - All workers listen on the same port (OS handles this)

3. **Request Distribution**
   - OS kernel distributes incoming connections
   - Round-robin algorithm ensures even distribution
   - No single worker gets overwhelmed

4. **Health Monitoring**
   - Primary monitors worker health via heartbeats
   - Detects crashes, hangs, and memory issues
   - Automatically spawns replacements for failed workers

5. **Graceful Shutdown**
   - Primary receives shutdown signal (SIGTERM/SIGINT)
   - Workers finish current requests
   - New requests rejected during shutdown
   - Clean process termination

<Callout type="info">
**Port Sharing**: All workers listen on the same port. The OS kernel handles distributing connections across workers using the SO_REUSEPORT socket option.
</Callout>

### Worker Lifecycle

```typescript
// This is what happens internally (you don't write this code)
if (config.server.cluster && cluster.isPrimary) {
  // Primary process - spawn workers
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }
  
  // Monitor workers
  cluster.on('exit', (worker, code, signal) => {
    console.log(`Worker ${worker.process.pid} died, starting replacement...`);
    cluster.fork(); // Auto-restart
  });
  
} else {
  // Worker process - run your application
  await app.listen();
}
```

<Callout type="success">
**Automatic Management**: You never write clustering code yourself. AzuraJS handles all the complexity internally based on your configuration.
</Callout>

## Advanced Configuration âš™ï¸

### Environment-Based Configuration

Enable cluster mode only in production:

```typescript title="azura.config.ts"
const isProduction = process.env.NODE_ENV === "production";
const isDevelopment = process.env.NODE_ENV === "development";

const config: ConfigTypes = {
  environment: isProduction ? "production" : "development",
  server: {
    port: Number(process.env.PORT) || 3000,
    cluster: isProduction, // Cluster only in production
  },
  logging: {
    enabled: true,
    showDetails: isDevelopment, // Verbose logs in dev only
  },
};

export default config;
```

### Custom Worker Count

Control the number of workers (advanced use case):

```typescript title="azura.config.ts"
import os from "os";

const config: ConfigTypes = {
  server: {
    port: 3000,
    cluster: true,
    // Note: AzuraJS automatically uses all cores
    // This is for documentation; custom worker count may be added in future
  },
};

export default config;
```

<Callout type="warn">
**Default Behavior**: Currently, AzuraJS spawns one worker per CPU core automatically. Custom worker count configuration may be added in future versions.
</Callout>

### Complete Production Configuration

```typescript title="azura.config.ts"
import type { ConfigTypes } from "azurajs/config";

const config: ConfigTypes = {
  environment: "production",
  server: {
    port: process.env.PORT || 3000,
    cluster: true,              // Enable cluster mode
    ipHost: false,              // Use 0.0.0.0 for all interfaces
  },
  logging: {
    enabled: true,
    showDetails: true,          // Show worker PIDs in logs
  },
  plugins: {
    cors: {
      enabled: true,
      origins: ["https://yourdomain.com"],
      methods: ["GET", "POST", "PUT", "DELETE", "PATCH"],
      credentials: true,
    },
  },
};

export default config;
```



## Shared State and Data Management ğŸ’¾

**Critical**: Workers run in **separate processes** and **do NOT share memory**. This is the most important concept to understand when using cluster mode.

### Understanding Process Isolation

Each worker is a completely separate Node.js process with its own:
- Memory space
- Variable scope
- Cache storage
- File descriptors
- Event loop

<Callout type="danger">
**Memory is NOT Shared**: Changes to variables in one worker are invisible to other workers. In-memory state must be stored externally.
</Callout>

### âŒ Anti-Patterns (Won't Work Across Workers)

#### In-Memory Cache
```typescript
// âŒ BAD: Each worker has its own cache
const cache = new Map();

@Get("/data/:id")
getData(@Param("id") id: string, @Res() res: ResponseServer) {
  if (cache.has(id)) {
    // Only works if same worker handles the request
    return res.json(cache.get(id));
  }
  
  const data = fetchFromDatabase(id);
  cache.set(id, data); // Stored only in THIS worker
  res.json(data);
}
```

**Problem**: Worker 1 caches the data, but Worker 2-4 won't see it. Every worker fetches from database independently.

#### In-Memory Session Storage
```typescript
// âŒ BAD: Sessions not shared between workers
const sessions = new Map<string, UserSession>();

app.use((req, res, next) => {
  const sessionId = req.cookies['session-id'];
  req.session = sessions.get(sessionId); // Might not exist on this worker
  next();
});
```

**Problem**: User logs in on Worker 1, next request hits Worker 2 which doesn't have the session.

#### Counters and Statistics
```typescript
// âŒ BAD: Each worker has separate counter
let requestCount = 0;

app.use((req, res, next) => {
  requestCount++; // Only increments in THIS worker
  next();
});

@Get("/stats")
getStats() {
  return { requests: requestCount }; // Shows only this worker's count
}
```

**Problem**: Total request count requires summing all workers, not possible with in-memory storage.

### âœ… Correct Patterns (Works with Cluster Mode)

#### Redis for Shared Cache

```typescript
import Redis from "ioredis";
const redis = new Redis({
  host: process.env.REDIS_HOST || "localhost",
  port: 6379,
  retryStrategy: (times) => Math.min(times * 50, 2000),
});

@Get("/data/:id")
async getData(@Param("id") id: string, @Res() res: ResponseServer) {
  // Check cache first
  const cached = await redis.get(`data:${id}`);
  if (cached) {
    return res.json(JSON.parse(cached));
  }
  
  // Fetch from database
  const data = await fetchFromDatabase(id);
  
  // Cache for all workers
  await redis.setex(`data:${id}`, 3600, JSON.stringify(data));
  
  res.json(data);
}
```

**Benefits**:
- âœ… All workers share the same cache
- âœ… Fast in-memory access via Redis
- âœ… Automatic expiration
- âœ… Persistence across restarts

#### Redis for Session Management

```typescript
import RedisStore from "connect-redis";
import session from "express-session";
import Redis from "ioredis";

const redisClient = new Redis();

app.use(session({
  store: new RedisStore({ client: redisClient }),
  secret: process.env.SESSION_SECRET,
  resave: false,
  saveUninitialized: false,
  cookie: {
    secure: process.env.NODE_ENV === "production",
    httpOnly: true,
    maxAge: 1000 * 60 * 60 * 24, // 24 hours
  },
}));

// Now sessions work across all workers
@Get("/profile")
getProfile(@Req() req: RequestServer, @Res() res: ResponseServer) {
  if (!req.session.userId) {
    return res.status(401).json({ error: "Not authenticated" });
  }
  
  res.json({ userId: req.session.userId });
}
```

#### Database for Persistent Data

```typescript
import { PrismaClient } from "@prisma/client";
const prisma = new PrismaClient();

@Post("/analytics/track")
async trackEvent(@Body() event: AnalyticsEvent, @Res() res: ResponseServer) {
  // Database is shared across all workers
  await prisma.analyticsEvent.create({
    data: {
      userId: event.userId,
      eventType: event.type,
      metadata: event.metadata,
      timestamp: new Date(),
    },
  });
  
  res.status(201).json({ success: true });
}

@Get("/analytics/stats")
async getStats(@Res() res: ResponseServer) {
  // Aggregate data from all workers' contributions
  const stats = await prisma.analyticsEvent.groupBy({
    by: ['eventType'],
    _count: { id: true },
  });
  
  res.json({ stats });
}
```

#### Message Queues for Worker Communication

```typescript
import Bull from "bull";

const emailQueue = new Bull("email", {
  redis: {
    host: "localhost",
    port: 6379,
  },
});

// Any worker can add jobs
@Post("/send-email")
async sendEmail(@Body() emailData: EmailData, @Res() res: ResponseServer) {
  await emailQueue.add("send", emailData);
  res.json({ status: "queued" });
}

// Jobs are processed by available workers
emailQueue.process("send", async (job) => {
  await sendEmailViaProvider(job.data);
});
```

### Recommended Solutions for Shared State

| Use Case | Solution | Library |
|----------|----------|---------|
| **Caching** | Redis | `ioredis`, `redis` |
| **Sessions** | Redis Store | `connect-redis` |
| **Database** | PostgreSQL/MySQL | `prisma`, `typeorm` |
| **Document Store** | MongoDB | `mongoose`, `mongodb` |
| **Job Queue** | Bull/BullMQ | `bull`, `bullmq` |
| **Pub/Sub** | Redis Pub/Sub | `ioredis` |
| **Real-time** | Redis Adapter | `socket.io-redis` |
| **File Storage** | S3/Cloud Storage | `@aws-sdk/client-s3` |

<Callout type="tip">
**Best Practice**: Design your application to be stateless. Store all state externally so any worker can handle any request.
</Callout>

### Hybrid Approach: Worker-Local Cache with Redis Fallback

```typescript
import Redis from "ioredis";
import NodeCache from "node-cache";

// L1 Cache: Worker-local (fast, not shared)
const localCache = new NodeCache({ stdTTL: 60 }); // 60 second TTL

// L2 Cache: Redis (shared across workers)
const redis = new Redis();

@Get("/product/:id")
async getProduct(@Param("id") id: string, @Res() res: ResponseServer) {
  // Try L1 cache first (fastest)
  let product = localCache.get<Product>(id);
  if (product) {
    return res.json({ product, source: "L1-cache" });
  }
  
  // Try L2 cache (Redis)
  const cached = await redis.get(`product:${id}`);
  if (cached) {
    product = JSON.parse(cached);
    localCache.set(id, product); // Populate L1 for next time
    return res.json({ product, source: "L2-cache" });
  }
  
  // Fetch from database
  product = await prisma.product.findUnique({ where: { id } });
  
  // Populate both caches
  localCache.set(id, product);
  await redis.setex(`product:${id}`, 300, JSON.stringify(product));
  
  res.json({ product, source: "database" });
}
```

**Benefits of Hybrid Approach**:
- âš¡ **Ultra-fast**: L1 cache serves in microseconds
- ğŸ”„ **Shared**: L2 cache reduces database load across all workers
- ğŸ“Š **Efficient**: Best of both worlds


## Performance Benefits and Benchmarks ğŸ“ˆ

Cluster mode provides significant performance improvements by utilizing all available CPU cores. Here's what you can expect:

### Expected Throughput Improvements

| CPU Cores | Throughput Increase | Use Case |
|-----------|---------------------|----------|
| 2 cores   | ~1.7-1.9x          | Small VPS, entry servers |
| 4 cores   | ~3.2-3.7x          | Standard cloud instances |
| 8 cores   | ~6.0-7.2x          | High-performance servers |
| 16 cores  | ~11-14x            | Dedicated servers |
| 32 cores  | ~20-26x            | Enterprise servers |

<Callout type="info">
**Note**: Actual performance gains depend on your application's characteristics, I/O patterns, and workload type.
</Callout>

### Factors Affecting Performance

#### I/O-Bound vs CPU-Bound

```typescript
// CPU-Bound Operations (benefit more from clustering)
@Get("/compute/:number")
async computeFactorial(@Param("number") n: string) {
  // Heavy CPU work
  let result = 1;
  for (let i = 1; i <= Number(n); i++) {
    result *= i;
  }
  return { result };
}

// I/O-Bound Operations (benefit less, but still useful)
@Get("/users/:id")
async getUser(@Param("id") id: string) {
  // I/O wait time dominates
  const user = await database.user.findUnique({
    where: { id },
    include: { posts: true, comments: true }
  });
  return { user };
}
```

**Performance Characteristics**:
- **CPU-Bound**: Near-linear scaling with cores (8 cores = ~7-8x performance)
- **I/O-Bound**: More modest gains (8 cores = ~2-4x performance)
- **Mixed Workload**: Typically see 4-6x improvement with 8 cores

### Real-World Benchmark Example

```bash
# Single Process (1 worker)
$ ab -n 10000 -c 100 http://localhost:3000/api/users
Requests per second:    1,250 [#/sec]

# Cluster Mode (8 workers on 8-core machine)
$ ab -n 10000 -c 100 http://localhost:3000/api/users
Requests per second:    7,800 [#/sec]
# 6.2x improvement!
```

### Memory Considerations

Each worker consumes memory, so monitor your usage:

```typescript
// Each worker loads the full application
@Controller("/api")
class ApiController {
  // This data is duplicated in each worker
  private readonly config = loadConfig();
  private readonly cache = new Map(); // Per-worker cache
}
```

**Memory Formula**:
```
Total Memory = Base Memory Ã— Number of Workers
```

**Example**: If one process uses 150MB:
- 4 workers = ~600MB
- 8 workers = ~1.2GB
- 16 workers = ~2.4GB

<Callout type="warn">
**Memory Planning**: Ensure your server has sufficient RAM. A good rule of thumb: `(Single Process Memory Ã— CPU Cores) + 20% buffer`
</Callout>

### Load Distribution

AzuraJS uses the OS kernel's round-robin load balancing:

```
Request Distribution Over Time:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ 25.3%
â”‚ Worker 2: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚ 24.8%
â”‚ Worker 3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚ 25.1%
â”‚ Worker 4: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚ 24.8%
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Container Orchestration (Docker, Kubernetes) ğŸ³

When using container orchestration platforms, **disable cluster mode** and let the orchestrator handle scaling.

### Why Disable Cluster Mode with Orchestration?

<Callout type="warn">
**Important**: Using cluster mode inside containers is redundant and can cause issues:
- Orchestrators already distribute load across containers
- Double-scaling adds unnecessary complexity
- Makes debugging and monitoring harder
- Consumes more resources than needed
</Callout>

### Recommended Configuration for Containers

```typescript title="azura.config.ts"
// Container environments: one process per container
const config: ConfigTypes = {
  server: {
    port: 3000,
    cluster: false,  // Disable - let orchestrator scale
    ipHost: false,   // Bind to 0.0.0.0 for container networking
  },
  logging: {
    enabled: true,
    showDetails: false, // Simpler logs for container aggregation
  },
};

export default config;
```

### Docker Compose Scaling

```yaml title="docker-compose.yml"
version: '3.8'

services:
  api:
    build: .
    image: myapp:latest
    environment:
      - NODE_ENV=production
      - PORT=3000
    deploy:
      replicas: 4  # Run 4 containers instead of cluster mode
      resources:
        limits:
          cpus: '1'     # Each container gets 1 CPU
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    depends_on:
      - api
```

### Kubernetes Deployment

```yaml title="k8s-deployment.yml"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: azurajs-app
  labels:
    app: azurajs
spec:
  replicas: 4  # Kubernetes manages 4 pods
  selector:
    matchLabels:
      app: azurajs
  template:
    metadata:
      labels:
        app: azurajs
    spec:
      containers:
      - name: app
        image: myapp:latest
        ports:
        - containerPort: 3000
        env:
        - name: NODE_ENV
          value: "production"
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
          limits:
            memory: "512Mi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: azurajs-service
spec:
  selector:
    app: azurajs
  ports:
  - protocol: TCP
    port: 80
    targetPort: 3000
  type: LoadBalancer
```

### Horizontal Pod Autoscaler

```yaml title="k8s-hpa.yml"
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: azurajs-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: azurajs-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### Comparison: Cluster Mode vs Container Orchestration

| Aspect | Cluster Mode | Container Orchestration |
|--------|--------------|------------------------|
| **Scaling Unit** | Process | Container/Pod |
| **Management** | AzuraJS | Kubernetes/Docker |
| **Resource Isolation** | Shared OS | Isolated containers |
| **Rolling Updates** | Manual | Built-in |
| **Health Checks** | Basic | Advanced |
| **Auto-scaling** | Fixed (CPU cores) | Dynamic (HPA) |
| **Best For** | Single server | Multi-server clusters |

## Monitoring and Observability ğŸ‘€

### Built-in Logging

Enable detailed logging to monitor worker behavior:

```typescript title="azura.config.ts"
const config: ConfigTypes = {
  server: {
    cluster: true,
  },
  logging: {
    enabled: true,
    showDetails: true,  // Shows worker PIDs and lifecycle events
  },
};
```

### Startup Logs

```bash
[Primary] Starting AzuraJS in cluster mode...
[Primary] Spawning 8 workers for 8 CPU cores
[Worker 1] Server listening on port 3000 (PID: 12345)
[Worker 2] Server listening on port 3000 (PID: 12346)
[Worker 3] Server listening on port 3000 (PID: 12347)
[Worker 4] Server listening on port 3000 (PID: 12348)
[Worker 5] Server listening on port 3000 (PID: 12349)
[Worker 6] Server listening on port 3000 (PID: 12350)
[Worker 7] Server listening on port 3000 (PID: 12351)
[Worker 8] Server listening on port 3000 (PID: 12352)
[Primary] All workers started successfully
```

### Worker Restart Logs

```bash
[Primary] Worker 3 (PID: 12347) crashed with code 1
[Primary] Starting replacement worker...
[Worker 9] Server listening on port 3000 (PID: 12360)
[Primary] Worker replacement successful
```

### Graceful Shutdown Logs

```bash
^C[Primary] Received SIGINT, initiating graceful shutdown...
[Primary] Sending shutdown signal to all workers
[Worker 1] Finishing current requests...
[Worker 2] Finishing current requests...
[Worker 3] Finishing current requests...
[Worker 4] Finishing current requests...
[Worker 1] Shut down gracefully (PID: 12345)
[Worker 2] Shut down gracefully (PID: 12346)
[Worker 3] Shut down gracefully (PID: 12347)
[Worker 4] Shut down gracefully (PID: 12348)
[Primary] All workers stopped. Exiting.
```

### Custom Worker Monitoring

```typescript
import os from "os";

@Get("/health")
async healthCheck(@Res() res: ResponseServer) {
  const memUsage = process.memoryUsage();
  
  res.json({
    status: "healthy",
    worker: {
      pid: process.pid,
      uptime: process.uptime(),
      memory: {
        rss: `${Math.round(memUsage.rss / 1024 / 1024)}MB`,
        heapUsed: `${Math.round(memUsage.heapUsed / 1024 / 1024)}MB`,
        heapTotal: `${Math.round(memUsage.heapTotal / 1024 / 1024)}MB`,
      },
    },
    system: {
      cpus: os.cpus().length,
      freeMem: `${Math.round(os.freemem() / 1024 / 1024)}MB`,
      totalMem: `${Math.round(os.totalmem() / 1024 / 1024)}MB`,
      loadAvg: os.loadavg(),
    },
  });
}
```

### Integration with Monitoring Tools

```typescript
// Prometheus metrics for cluster monitoring
import promClient from "prom-client";

// Create a histogram for request duration per worker
const httpRequestDuration = new promClient.Histogram({
  name: "http_request_duration_seconds",
  help: "Duration of HTTP requests in seconds",
  labelNames: ["method", "route", "status_code", "worker_id"],
});

app.use((req, res, next) => {
  const start = Date.now();
  
  res.on("finish", () => {
    const duration = (Date.now() - start) / 1000;
    httpRequestDuration
      .labels(req.method, req.route?.path || req.url, res.statusCode.toString(), process.pid.toString())
      .observe(duration);
  });
  
  next();
});

@Get("/metrics")
async getMetrics(@Res() res: ResponseServer) {
  res.set("Content-Type", promClient.register.contentType);
  res.send(await promClient.register.metrics());
}
```


## Best Practices and Recommendations âœ¨

### Development vs Production Strategy

```typescript title="azura.config.ts"
const isDevelopment = process.env.NODE_ENV === "development";
const isProduction = process.env.NODE_ENV === "production";

const config: ConfigTypes = {
  environment: process.env.NODE_ENV || "development",
  server: {
    port: Number(process.env.PORT) || 3000,
    cluster: isProduction && !process.env.DISABLE_CLUSTER, // Flexible control
  },
  logging: {
    enabled: true,
    showDetails: isDevelopment, // Verbose in dev, concise in prod
  },
};

export default config;
```

<Callout type="tip">
**Development**: Keep `cluster: false` for easier debugging with a single process. Hot reload and breakpoints work better.
</Callout>

<Callout type="success">
**Production**: Enable `cluster: true` to maximize performance and reliability on multi-core servers.
</Callout>

### Design for Statelessness

```typescript
// âœ… GOOD: Stateless design
@Controller("/api/cart")
class CartController {
  constructor(private readonly redis: Redis) {}
  
  @Post("/items")
  async addItem(@Body() item: CartItem, @Req() req: RequestServer) {
    const userId = req.user.id;
    
    // Store in Redis (available to all workers)
    await this.redis.sadd(`cart:${userId}`, JSON.stringify(item));
    
    return { success: true };
  }
  
  @Get("/items")
  async getItems(@Req() req: RequestServer) {
    const userId = req.user.id;
    
    // Retrieve from Redis
    const items = await this.redis.smembers(`cart:${userId}`);
    return { items: items.map(JSON.parse) };
  }
}

// âŒ BAD: Stateful design
const userCarts = new Map<string, CartItem[]>(); // Won't work across workers

@Post("/items")
addItem(@Body() item: CartItem, @Req() req: RequestServer) {
  const userId = req.user.id;
  const cart = userCarts.get(userId) || [];
  cart.push(item);
  userCarts.set(userId, cart); // Only in this worker!
}
```

### External Storage Best Practices

```typescript
// Connection pooling for database
import { PrismaClient } from "@prisma/client";

// Create one client per worker (not per request!)
const prisma = new PrismaClient({
  datasources: {
    db: {
      url: process.env.DATABASE_URL,
    },
  },
  log: process.env.NODE_ENV === "development" ? ["query", "error"] : ["error"],
});

// Redis with automatic reconnection
import Redis from "ioredis";

const redis = new Redis({
  host: process.env.REDIS_HOST || "localhost",
  port: Number(process.env.REDIS_PORT) || 6379,
  password: process.env.REDIS_PASSWORD,
  retryStrategy: (times) => {
    const delay = Math.min(times * 50, 2000);
    return delay;
  },
  maxRetriesPerRequest: 3,
});

// Handle Redis connection events
redis.on("error", (err) => {
  console.error("Redis connection error:", err);
});

redis.on("connect", () => {
  console.log(`[Worker ${process.pid}] Connected to Redis`);
});
```

### Health Check Implementation

```typescript
@Get("/health")
async healthCheck(@Res() res: ResponseServer) {
  const checks = {
    status: "healthy",
    timestamp: new Date().toISOString(),
    worker: {
      pid: process.pid,
      uptime: Math.floor(process.uptime()),
    },
  };
  
  // Check Redis connectivity
  try {
    await redis.ping();
    checks.redis = "connected";
  } catch (error) {
    checks.redis = "disconnected";
    checks.status = "unhealthy";
  }
  
  // Check database connectivity
  try {
    await prisma.$queryRaw`SELECT 1`;
    checks.database = "connected";
  } catch (error) {
    checks.database = "disconnected";
    checks.status = "unhealthy";
  }
  
  const statusCode = checks.status === "healthy" ? 200 : 503;
  res.status(statusCode).json(checks);
}
```

### Graceful Error Handling

```typescript
// Global error handler
app.use((error, req, res, next) => {
  console.error(`[Worker ${process.pid}] Error:`, error);
  
  res.status(error.status || 500).json({
    error: {
      message: process.env.NODE_ENV === "production" 
        ? "Internal server error" 
        : error.message,
      worker: process.pid,
    },
  });
});

// Uncaught exception handler
process.on("uncaughtException", (error) => {
  console.error(`[Worker ${process.pid}] Uncaught exception:`, error);
  // Let AzuraJS restart this worker
  process.exit(1);
});

// Unhandled promise rejection
process.on("unhandledRejection", (reason, promise) => {
  console.error(`[Worker ${process.pid}] Unhandled rejection:`, reason);
  // Optionally exit to trigger restart
  process.exit(1);
});
```

## Troubleshooting Common Issues ğŸ”

### Issue: Workers Keep Crashing

**Symptoms**:
```bash
[Primary] Worker 3 (PID: 12347) crashed with code 1
[Primary] Worker 5 (PID: 12360) crashed with code 1
[Primary] Worker 7 (PID: 12375) crashed with code 1
```

**Common Causes**:
1. **Uncaught Exceptions**: Unhandled errors causing process termination
2. **Memory Leaks**: Worker runs out of memory
3. **Database Issues**: Connection problems or query failures
4. **Missing Dependencies**: Required modules not installed

**Solutions**:
```typescript
// Add comprehensive error handling
process.on("uncaughtException", (error) => {
  console.error("Uncaught Exception:", error);
  // Log to external service
  logger.error("Uncaught exception", { error, pid: process.pid });
  // Gracefully shutdown
  setTimeout(() => process.exit(1), 1000);
});

process.on("unhandledRejection", (reason) => {
  console.error("Unhandled Rejection:", reason);
  logger.error("Unhandled rejection", { reason, pid: process.pid });
});

// Wrap async operations
@Get("/data")
async getData(@Res() res: ResponseServer) {
  try {
    const data = await fetchFromDatabase();
    res.json(data);
  } catch (error) {
    console.error("Error fetching data:", error);
    res.status(500).json({ error: "Failed to fetch data" });
  }
}
```

### Issue: Inconsistent Behavior Between Requests

**Symptoms**: Same request returns different results or user session lost

**Cause**: Using in-memory state that's not shared between workers

**Solution**: Move all state to external storage

```typescript
// âŒ Problem Code
const cache = new Map();

@Get("/config")
getConfig() {
  if (!cache.has("config")) {
    cache.set("config", loadConfig()); // Only in this worker
  }
  return cache.get("config");
}

// âœ… Fixed Code
import Redis from "ioredis";
const redis = new Redis();

@Get("/config")
async getConfig() {
  let config = await redis.get("config");
  if (!config) {
    config = JSON.stringify(loadConfig());
    await redis.setex("config", 3600, config); // Shared across workers
  }
  return JSON.parse(config);
}
```

### Issue: Port Already in Use

**Symptoms**:
```bash
Error: listen EADDRINUSE: address already in use :::3000
```

**Causes**:
1. Another application using the port
2. Multiple AzuraJS instances running
3. Previous instance didn't shut down cleanly

**Solutions**:
```bash
# Check what's using the port
lsof -i :3000          # macOS/Linux
netstat -ano | findstr :3000  # Windows

# Kill the process
kill -9 <PID>          # macOS/Linux
taskkill /PID <PID> /F # Windows

# Or use a different port
PORT=3001 npm start
```

### Issue: High Memory Usage

**Symptoms**: Memory consumption grows over time, system becomes sluggish

**Causes**:
1. Memory leaks in application code
2. Too many workers for available RAM
3. Large in-memory caches

**Solutions**:
```typescript
// Monitor memory usage
@Get("/metrics/memory")
getMemoryMetrics() {
  const usage = process.memoryUsage();
  return {
    worker: process.pid,
    rss: `${Math.round(usage.rss / 1024 / 1024)}MB`,
    heapUsed: `${Math.round(usage.heapUsed / 1024 / 1024)}MB`,
    heapTotal: `${Math.round(usage.heapTotal / 1024 / 1024)}MB`,
    external: `${Math.round(usage.external / 1024 / 1024)}MB`,
  };
}

// Implement memory limits
import v8 from "v8";

const heapStats = v8.getHeapStatistics();
const maxHeap = heapStats.heap_size_limit;

setInterval(() => {
  const usage = process.memoryUsage();
  if (usage.heapUsed > maxHeap * 0.9) {
    console.warn(`Worker ${process.pid} approaching memory limit`);
    // Trigger garbage collection if needed
    if (global.gc) {
      global.gc();
    }
  }
}, 30000); // Check every 30 seconds
```

### Issue: Database Connection Pool Exhausted

**Symptoms**:
```bash
Error: Connection pool timeout
Error: Too many connections
```

**Cause**: Each worker creates its own database connections

**Solution**: Adjust connection pool size

```typescript
// Calculate appropriate pool size
const numWorkers = require("os").cpus().length;
const connectionsPerWorker = 10; // Adjust based on your needs
const totalConnections = numWorkers * connectionsPerWorker;

// Configure Prisma
const prisma = new PrismaClient({
  datasources: {
    db: {
      url: process.env.DATABASE_URL,
    },
  },
  // Prisma automatically manages pool size, but you can tune it
  log: ["error"],
});

// For other ORMs like TypeORM
const dataSource = new DataSource({
  type: "postgres",
  host: "localhost",
  port: 5432,
  database: "mydb",
  poolSize: 10, // Per worker
  // Total connections = poolSize Ã— numWorkers
});

// Ensure database can handle total connections
// PostgreSQL max_connections should be > total connections
```

## Performance Tuning Tips âš¡

### 1. Optimize Worker Count

```typescript
import os from "os";

// Default: one worker per core
const numCPUs = os.cpus().length;

// For I/O-heavy apps: may benefit from more workers
const ioHeavyWorkers = numCPUs * 2;

// For CPU-heavy apps: stick to core count
const cpuHeavyWorkers = numCPUs;

// For memory-constrained systems: fewer workers
const availableMemoryMB = os.totalmem() / 1024 / 1024;
const memoryPerWorkerMB = 512; // Estimate
const maxWorkersByMemory = Math.floor(availableMemoryMB / memoryPerWorkerMB);
```

### 2. Enable HTTP Keep-Alive

```typescript
import http from "http";

// Configure keep-alive
const server = http.createServer((req, res) => {
  res.setHeader("Connection", "keep-alive");
  res.setHeader("Keep-Alive", "timeout=5, max=1000");
  // Your handler
});

server.keepAliveTimeout = 5000; // 5 seconds
server.headersTimeout = 6000; // Must be > keepAliveTimeout
```

### 3. Use Caching Strategically

```typescript
// Example: Cache expensive computations
import NodeCache from "node-cache";

const cache = new NodeCache({ 
  stdTTL: 600, // 10 minutes
  checkperiod: 120, // Check for expired keys every 2 minutes
  useClones: false, // Better performance, but modify with care
});

@Get("/expensive-data")
async getExpensiveData(@Res() res: ResponseServer) {
  const cacheKey = "expensive_data";
  
  // Check worker-local cache first
  let data = cache.get(cacheKey);
  if (data) {
    return res.json({ data, cached: true });
  }
  
  // Compute expensive result
  data = await performExpensiveOperation();
  cache.set(cacheKey, data);
  
  res.json({ data, cached: false });
}
```

### 4. Database Query Optimization

```typescript
// Use connection pooling
// Implement query result caching
// Use database indexes

@Get("/users/:id")
async getUser(@Param("id") id: string) {
  // Cache database queries in Redis
  const cached = await redis.get(`user:${id}`);
  if (cached) return JSON.parse(cached);
  
  const user = await prisma.user.findUnique({
    where: { id },
    select: { 
      id: true, 
      name: true, 
      email: true,
      // Only select needed fields
    },
  });
  
  await redis.setex(`user:${id}`, 300, JSON.stringify(user));
  return user;
}
```

## Next Steps and Resources ğŸ“–

<Cards>
  <Card 
    title="Configuration" 
    href="/docs/en/configuration" 
    description="Explore all configuration options for fine-tuning"
  />
  <Card 
    title="Performance Optimization" 
    href="/docs/en/performance" 
    description="Advanced techniques to maximize performance"
  />
  <Card 
    title="Error Handling" 
    href="/docs/en/error-handling" 
    description="Implement robust error handling strategies"
  />
  <Card 
    title="Deployment Guide" 
    href="/docs/en/deployment" 
    description="Deploy to production with confidence"
  />
</Cards>

### Additional Resources

- **Redis Documentation**: https://redis.io/docs/
- **Node.js Cluster Module**: https://nodejs.org/api/cluster.html
- **Kubernetes Best Practices**: https://kubernetes.io/docs/concepts/configuration/
- **Load Balancing Strategies**: https://nginx.org/en/docs/http/load_balancing.html

### Key Takeaways

1. âœ… **Enable in Production**: Use cluster mode on multi-core production servers
2. âœ… **External Storage**: Always store shared state in Redis/databases
3. âœ… **Monitor Workers**: Track memory, CPU, and health of each worker
4. âœ… **Design Stateless**: Build applications that work across any worker
5. âœ… **Container Awareness**: Disable when using Kubernetes/Docker Swarm
6. âœ… **Test Thoroughly**: Ensure your application works correctly in cluster mode

<Callout type="success">
**Congratulations!** You now understand how to effectively use AzuraJS cluster mode to build high-performance, scalable applications. Remember: cluster mode is powerful but requires proper state management and monitoring.
</Callout>
