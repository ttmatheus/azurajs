---
title: Performance
description: Optimize your AzuraJS application for maximum performance
icon: Zap
---

# Performance ‚ö°

Optimize your AzuraJS application for speed, efficiency, and scalability.

## Built-in Optimizations üöÄ

AzuraJS is designed for performance:

- ‚úÖ **Zero dependencies** - No bloated node_modules
- ‚úÖ **Efficient routing** - Fast route matching algorithm
- ‚úÖ **Minimal overhead** - Direct Node.js HTTP server integration
- ‚úÖ **Cluster mode** - Multi-core CPU utilization
- ‚úÖ **Streaming support** - Handle large payloads efficiently

## Cluster Mode üñ•Ô∏è

Simply enable cluster mode in your configuration - AzuraJS handles everything automatically:

```typescript title="azura.config.ts"
const config: ConfigTypes = {
  server: {
    cluster: true,  // AzuraJS automatically uses all CPU cores
  },
};
```

**Performance gain**: ~7x throughput on 8-core systems

**No manual code needed** - AzuraJS automatically creates workers, distributes load, and handles crashes.

See the [Cluster Mode guide](/docs/en/cluster-mode) for complete details.

## Strategic Caching üíæ

### In-Memory Cache

```typescript
const cache = new Map<string, { data: any; expires: number }>();

function cacheMiddleware(ttl: number) {
  return (req: RequestServer, res: ResponseServer, next: () => void) => {
    if (req.method !== "GET") {
      return next();
    }
    
    const key = req.url;
    const cached = cache.get(key);
    
    // Check cache
    if (cached && Date.now() < cached.expires) {
      return res.json(cached.data);
    }
    
    // Intercept response
    const originalJson = res.json.bind(res);
    res.json = function(data: any) {
      cache.set(key, { data, expires: Date.now() + ttl });
      return originalJson(data);
    };
    
    next();
  };
}

// 5 minute cache for public data
app.use("/api/public", cacheMiddleware(300000));
```

### Redis Cache

```typescript
import { createClient } from "redis";

const redis = createClient();
await redis.connect();

async function redisCacheMiddleware(
  req: RequestServer,
  res: ResponseServer,
  next: () => void
) {
  if (req.method !== "GET") {
    return next();
  }
  
  const key = `cache:${req.url}`;
  
  try {
    const cached = await redis.get(key);
    
    if (cached) {
      return res.json(JSON.parse(cached));
    }
    
    const originalJson = res.json.bind(res);
    res.json = async function(data: any) {
      await redis.setEx(key, 300, JSON.stringify(data));
      return originalJson(data);
    };
    
    next();
  } catch (error) {
    next();  // Fail open
  }
}

app.use(redisCacheMiddleware);
```

## Database Optimization üóÑÔ∏è

### Connection Pooling

```typescript
import { Pool } from "pg";

const pool = new Pool({
  host: "localhost",
  port: 5432,
  database: "mydb",
  user: "user",
  password: "password",
  max: 20,  // Maximum 20 connections
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000
});

@Get("/users")
async getUsers() {
  const client = await pool.connect();
  try {
    const result = await client.query("SELECT * FROM users");
    return { users: result.rows };
  } finally {
    client.release();
  }
}
```

### Query Optimization

```typescript
// ‚ùå N+1 Query Problem
async function getBadPosts() {
  const posts = await db.query("SELECT * FROM posts");
  
  for (const post of posts) {
    post.author = await db.query("SELECT * FROM users WHERE id = $1", [post.authorId]);
  }
  
  return posts;
}

// ‚úÖ Single Query with JOIN
async function getGoodPosts() {
  const posts = await db.query(`
    SELECT posts.*, users.name as author_name, users.email as author_email
    FROM posts
    LEFT JOIN users ON posts.author_id = users.id
  `);
  
  return posts;
}
```

### Indexes

```sql
-- Create indexes for frequent queries
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_posts_author_id ON posts(author_id);
CREATE INDEX idx_posts_created_at ON posts(created_at DESC);

-- Composite index for specific queries
CREATE INDEX idx_posts_status_created ON posts(status, created_at DESC);
```

## Efficient Pagination üìÑ

```typescript
interface PaginationQuery {
  page: number;
  limit: number;
  sortBy?: string;
  sortOrder?: "asc" | "desc";
}

@Get("/posts")
async getPosts(@Query() query: PaginationQuery) {
  const page = Math.max(1, query.page || 1);
  const limit = Math.min(100, Math.max(1, query.limit || 10));
  const offset = (page - 1) * limit;
  const sortBy = query.sortBy || "created_at";
  const sortOrder = query.sortOrder || "desc";
  
  // Query with LIMIT and OFFSET
  const posts = await db.query(`
    SELECT * FROM posts
    ORDER BY ${sortBy} ${sortOrder}
    LIMIT $1 OFFSET $2
  `, [limit, offset]);
  
  // Count total (can be cached)
  const total = await db.query("SELECT COUNT(*) FROM posts");
  
  return {
    posts,
    pagination: {
      page,
      limit,
      total: total.rows[0].count,
      totalPages: Math.ceil(total.rows[0].count / limit)
    }
  };
}
```

## Compression üóÅÔ∏è

```typescript
import { gzipSync } from "zlib";

function compressionMiddleware(req: RequestServer, res: ResponseServer, next: () => void) {
  const acceptEncoding = req.headers["accept-encoding"] || "";
  
  if (!acceptEncoding.includes("gzip")) {
    return next();
  }
  
  const originalJson = res.json.bind(res);
  const originalSend = res.send.bind(res);
  
  res.json = function(data: any) {
    const json = JSON.stringify(data);
    const compressed = gzipSync(json);
    
    res.setHeader("Content-Encoding", "gzip");
    res.setHeader("Content-Type", "application/json");
    return originalSend(compressed);
  };
  
  next();
}

app.use(compressionMiddleware);
```

## Lazy Loading üîÑ

```typescript
// Load only what's needed
@Get("/posts/:id")
async getPost(@Param("id") id: string, @Query("include") include: string) {
  const post = await db.query("SELECT * FROM posts WHERE id = $1", [id]);
  
  // Load relationships only if requested
  if (include?.includes("author")) {
    post.author = await db.query("SELECT * FROM users WHERE id = $1", [post.authorId]);
  }
  
  if (include?.includes("comments")) {
    post.comments = await db.query("SELECT * FROM comments WHERE post_id = $1", [post.id]);
  }
  
  return { post };
}
```

## Smart Rate Limiting üõ°Ô∏è

```typescript
// Higher rate limit for authenticated users
function smartRateLimit(req: RequestServer, res: ResponseServer, next: () => void) {
  const isAuthenticated = !!req.user;
  
  const limit = isAuthenticated ? 1000 : 100;  // 10x more for authenticated users
  const windowMs = 60000;  // 1 minute
  
  const key = isAuthenticated ? `auth:${req.user.id}` : `anon:${req.ip}`;
  
  // Implement rate limiting with dynamic limit
  const allowed = checkRateLimit(key, windowMs, limit);
  
  if (!allowed) {
    return res.status(429).json({ error: "Rate limit exceeded" });
  }
  
  next();
}
```

## Async/Await vs Promises.all üîÄ

```typescript
// ‚ùå Slow - Sequential execution
async function slowFetch() {
  const users = await fetchUsers();      // Wait 100ms
  const posts = await fetchPosts();      // Wait 100ms
  const comments = await fetchComments();  // Wait 100ms
  // Total: 300ms
  
  return { users, posts, comments };
}

// ‚úÖ Fast - Parallel execution
async function fastFetch() {
  const [users, posts, comments] = await Promise.all([
    fetchUsers(),      // Execute in parallel
    fetchPosts(),      // Execute in parallel
    fetchComments()    // Execute in parallel
  ]);
  // Total: 100ms (time of slowest operation)
  
  return { users, posts, comments };
}
```

## Streaming üåä

For large responses, use streaming:

```typescript
import { createReadStream } from "fs";

@Get("/download/large-file")
downloadFile(@Res() res: ResponseServer) {
  const stream = createReadStream("./large-file.csv");
  
  res.setHeader("Content-Type", "text/csv");
  res.setHeader("Content-Disposition", "attachment; filename=data.csv");
  
  stream.pipe(res);
}
```

## Memory Management üß†

### Avoid Memory Leaks

```typescript
// ‚ùå Memory leak: unbounded cache
const cache = new Map();

@Get("/data")
getData(@Query("key") key: string, @Res() res: ResponseServer) {
  cache.set(key, data);  // Never cleaned up!
  res.json({ data });
}

// ‚úÖ Fixed: LRU cache with max size
class LRUCache<K, V> {
  private cache = new Map<K, V>();
  
  constructor(private maxSize: number) {}
  
  set(key: K, value: V) {
    if (this.cache.size >= this.maxSize) {
      const firstKey = this.cache.keys().next().value;
      this.cache.delete(firstKey);
    }
    this.cache.set(key, value);
  }
  
  get(key: K): V | undefined {
    return this.cache.get(key);
  }
}

const cache = new LRUCache(1000);  // Max 1000 entries
```

### Clean Up Resources

```typescript
@Post("/process")
async processFile(@Body() data: any, @Res() res: ResponseServer) {
  let tempFile: string | null = null;
  
  try {
    tempFile = await saveTempFile(data);
    const result = await processFile(tempFile);
    res.json({ result });
  } finally {
    // Always clean up
    if (tempFile) {
      await fs.unlink(tempFile);
    }
  }
}
```

## Benchmarking üìä

Use tools to measure performance:

```bash
# Autocannon (Node.js)
npm install -g autocannon
autocannon -c 100 -d 10 http://localhost:3000/api/users

# Apache Bench
ab -n 10000 -c 100 http://localhost:3000/api/users

# wrk
wrk -t12 -c400 -d30s http://localhost:3000/api/users
```

### Benchmark Example

```typescript
import autocannon from "autocannon";

async function benchmark() {
  const result = await autocannon({
    url: "http://localhost:3000",
    connections: 100,
    duration: 10,
    pipelining: 1,
    requests: [
      {
        method: "GET",
        path: "/api/users"
      }
    ]
  });
  
  console.log(`Requests/sec: ${result.requests.mean}`);
  console.log(`Latency (avg): ${result.latency.mean}ms`);
  console.log(`Throughput: ${result.throughput.mean} bytes/sec`);
}

benchmark();
```

## Monitoring üìà

```typescript
import { performance } from "perf_hooks";

function performanceMiddleware(req: RequestServer, res: ResponseServer, next: () => void) {
  const start = performance.now();
  
  res.on("finish", () => {
    const duration = performance.now() - start;
    
    console.log({
      method: req.method,
      url: req.url,
      statusCode: res.statusCode,
      duration: `${duration.toFixed(2)}ms`
    });
    
    // Alert if request takes too long
    if (duration > 1000) {
      console.warn(`‚ö†Ô∏è Slow request: ${req.method} ${req.url} took ${duration.toFixed(2)}ms`);
    }
  });
  
  next();
}

app.use(performanceMiddleware);
```

## Production Optimizations üöÄ

### Environment Variables

```typescript
const app = new AzuraClient({
  environment: "production",
  logging: {
    level: process.env.NODE_ENV === "production" ? "error" : "debug"
  }
});
```

### PM2 Configuration

```javascript
// ecosystem.config.js
module.exports = {
  apps: [{
    name: "azura-app",
    script: "./dist/server.js",
    instances: "max",  // Use all cores
    exec_mode: "cluster",
    env_production: {
      NODE_ENV: "production",
      PORT: 3000
    },
    max_memory_restart: "500M",
    error_file: "./logs/error.log",
    out_file: "./logs/out.log",
    log_date_format: "YYYY-MM-DD HH:mm:ss Z"
  }]
};
```

## Performance Checklist ‚úÖ

<Callout type="tip">
  ‚úÖ **Use cluster mode** to utilize all CPU cores
</Callout>

<Callout type="tip">
  ‚úÖ **Implement caching** for data that doesn't change frequently
</Callout>

<Callout type="tip">
  ‚úÖ **Optimize database queries** with proper indexes and JOINs
</Callout>

<Callout type="tip">
  ‚úÖ **Use connection pooling** for database
</Callout>

<Callout type="tip">
  ‚úÖ **Compress responses** with gzip
</Callout>

<Callout type="tip">
  ‚úÖ **Paginate results** to limit transferred data
</Callout>

<Callout type="tip">
  ‚úÖ **Use Promise.all** for parallel operations
</Callout>

<Callout type="tip">
  ‚úÖ **Monitor performance** with metrics and logs
</Callout>

<Callout type="warn">
  ‚ö†Ô∏è **Avoid event loop blocking** with heavy synchronous operations
</Callout>

<Callout type="warn">
  ‚ö†Ô∏è **Don't do N+1 queries** - always use JOINs or batch queries
</Callout>

## Performance Comparison üìä

**Single server vs Cluster (8 cores):**

```
Single Process:
  Requests/sec: 5,000
  Latency: 20ms

Cluster Mode (8 cores):
  Requests/sec: 35,000
  Latency: 3ms
  
Gain: 7x throughput, 6.6x latency
```

## Next Steps üìñ

<Cards>
  <Card title="Cluster Mode" href="cluster-mode" description="Scale across multiple cores" />
  <Card title="Rate Limiting" href="rate-limiting" description="Protect against abuse" />
  <Card title="Examples" href="examples" description="See optimized examples" />
</Cards>
Configure cluster mode" />
  <Card title="Examples" href="examples" description="See complete implementations" />
  <Card title="Error Handling" href="error-handling" description="Handle errors efficiently