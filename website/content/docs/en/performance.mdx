---
title: Performance
description: Optimize your AzuraJS application for maximum performance
icon: Zap
---

# Performance ‚ö°

Optimize your AzuraJS application for speed, efficiency, and scalability.

## Built-in Optimizations üöÄ

AzuraJS is designed for performance:

- ‚úÖ **Zero dependencies** - No bloated node_modules
- ‚úÖ **Efficient routing** - Fast route matching algorithm
- ‚úÖ **Minimal overhead** - Direct Node.js HTTP server integration
- ‚úÖ **Cluster mode** - Multi-core CPU utilization
- ‚úÖ **Streaming support** - Handle large payloads efficiently

## Cluster Mode üñ•Ô∏è

Simply enable cluster mode in your configuration - AzuraJS handles everything automatically:

```typescript title="azura.config.ts"
const config: ConfigTypes = {
  server: {
    cluster: true,  // AzuraJS automatically uses all CPU cores
  },
};
```

**Performance gain**: ~7x throughput on 8-core systems

**No manual code needed** - AzuraJS automatically creates workers, distributes load, and handles crashes.

See the [Cluster Mode guide](/docs/en/cluster-mode) for complete details.

## Strategic Caching üíæ

### In-Memory Cache

```typescript
const cache = new Map<string, { data: any; expires: number }>();

function cacheMiddleware(ttl: number) {
  return (req: RequestServer, res: ResponseServer, next: () => void) => {
    if (req.method !== "GET") {
      return next();
    }
    
    const key = req.url;
    const cached = cache.get(key);
    
    // Check cache
    if (cached && Date.now() < cached.expires) {
      return res.json(cached.data);
    }
    
    // Intercept response
    const originalJson = res.json.bind(res);
    res.json = function(data: any) {
      cache.set(key, { data, expires: Date.now() + ttl });
      return originalJson(data);
    };
    
    next();
  };
}

// 5 minute cache for public data
app.use("/api/public", cacheMiddleware(300000));
```

### Redis Cache

```typescript
import { createClient } from "redis";

const redis = createClient();
await redis.connect();

async function redisCacheMiddleware(
  req: RequestServer,
  res: ResponseServer,
  next: () => void
) {
  if (req.method !== "GET") {
    return next();
  }
  
  const key = `cache:${req.url}`;
  
  try {
    const cached = await redis.get(key);
    
    if (cached) {
      return res.json(JSON.parse(cached));
    }
    
    const originalJson = res.json.bind(res);
    res.json = async function(data: any) {
      await redis.setEx(key, 300, JSON.stringify(data));
      return originalJson(data);
    };
    
    next();
  } catch (error) {
    next();  // Fail open
  }
}

app.use(redisCacheMiddleware);
```

## Database Optimization üóÑÔ∏è

### Connection Pooling

```typescript
import { Pool } from "pg";

const pool = new Pool({
  host: "localhost",
  port: 5432,
  database: "mydb",
  user: "user",
  password: "password",
  max: 20,  // Maximum 20 connections
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000
});

@Get("/users")
async getUsers() {
  const client = await pool.connect();
  try {
    const result = await client.query("SELECT * FROM users");
    return { users: result.rows };
  } finally {
    client.release();
  }
}
```

### Query Optimization

```typescript
// ‚ùå N+1 Query Problem
async function getBadPosts() {
  const posts = await db.query("SELECT * FROM posts");
  
  for (const post of posts) {
    post.author = await db.query("SELECT * FROM users WHERE id = $1", [post.authorId]);
  }
  
  return posts;
}

// ‚úÖ Single Query with JOIN
async function getGoodPosts() {
  const posts = await db.query(`
    SELECT posts.*, users.name as author_name, users.email as author_email
    FROM posts
    LEFT JOIN users ON posts.author_id = users.id
  `);
  
  return posts;
}
```

### Indexes

```sql
-- Create indexes for frequent queries
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_posts_author_id ON posts(author_id);
CREATE INDEX idx_posts_created_at ON posts(created_at DESC);

-- Composite index for specific queries
CREATE INDEX idx_posts_status_created ON posts(status, created_at DESC);
```

## Efficient Pagination üìÑ

```typescript
interface PaginationQuery {
  page: number;
  limit: number;
  sortBy?: string;
  sortOrder?: "asc" | "desc";
}

@Get("/posts")
async getPosts(@Query() query: PaginationQuery) {
  const page = Math.max(1, query.page || 1);
  const limit = Math.min(100, Math.max(1, query.limit || 10));
  const offset = (page - 1) * limit;
  const sortBy = query.sortBy || "created_at";
  const sortOrder = query.sortOrder || "desc";
  
  // Query with LIMIT and OFFSET
  const posts = await db.query(`
    SELECT * FROM posts
    ORDER BY ${sortBy} ${sortOrder}
    LIMIT $1 OFFSET $2
  `, [limit, offset]);
  
  // Count total (can be cached)
  const total = await db.query("SELECT COUNT(*) FROM posts");
  
  return {
    posts,
    pagination: {
      page,
      limit,
      total: total.rows[0].count,
      totalPages: Math.ceil(total.rows[0].count / limit)
    }
  };
}
```

## Compression üóÅÔ∏è

```typescript
import { gzipSync } from "zlib";

function compressionMiddleware(req: RequestServer, res: ResponseServer, next: () => void) {
  const acceptEncoding = req.headers["accept-encoding"] || "";
  
  if (!acceptEncoding.includes("gzip")) {
    return next();
  }
  
  const originalJson = res.json.bind(res);
  const originalSend = res.send.bind(res);
  
  res.json = function(data: any) {
    const json = JSON.stringify(data);
    const compressed = gzipSync(json);
    
    res.setHeader("Content-Encoding", "gzip");
    res.setHeader("Content-Type", "application/json");
    return originalSend(compressed);
  };
  
  next();
}

app.use(compressionMiddleware);
```

## Lazy Loading üîÑ

```typescript
// Load only what's needed
@Get("/posts/:id")
async getPost(@Param("id") id: string, @Query("include") include: string) {
  const post = await db.query("SELECT * FROM posts WHERE id = $1", [id]);
  
  // Load relationships only if requested
  if (include?.includes("author")) {
    post.author = await db.query("SELECT * FROM users WHERE id = $1", [post.authorId]);
  }
  
  if (include?.includes("comments")) {
    post.comments = await db.query("SELECT * FROM comments WHERE post_id = $1", [post.id]);
  }
  
  return { post };
}
```

## Smart Rate Limiting üõ°Ô∏è

```typescript
// Higher rate limit for authenticated users
function smartRateLimit(req: RequestServer, res: ResponseServer, next: () => void) {
  const isAuthenticated = !!req.user;
  
  const limit = isAuthenticated ? 1000 : 100;  // 10x more for authenticated users
  const windowMs = 60000;  // 1 minute
  
  const key = isAuthenticated ? `auth:${req.user.id}` : `anon:${req.ip}`;
  
  // Implement rate limiting with dynamic limit
  const allowed = checkRateLimit(key, windowMs, limit);
  
  if (!allowed) {
    return res.status(429).json({ error: "Rate limit exceeded" });
  }
  
  next();
}
```

## Async/Await vs Promises.all üîÄ

```typescript
// ‚ùå Slow - Sequential execution
async function slowFetch() {
  const users = await fetchUsers();      // Wait 100ms
  const posts = await fetchPosts();      // Wait 100ms
  const comments = await fetchComments();  // Wait 100ms
  // Total: 300ms
  
  return { users, posts, comments };
}

// ‚úÖ Fast - Parallel execution
async function fastFetch() {
  const [users, posts, comments] = await Promise.all([
    fetchUsers(),      // Execute in parallel
    fetchPosts(),      // Execute in parallel
    fetchComments()    // Execute in parallel
  ]);
  // Total: 100ms (time of slowest operation)
  
  return { users, posts, comments };
}
```

## Streaming üåä

For large responses, use streaming:

```typescript
import { createReadStream } from "fs";

@Get("/download/large-file")
downloadFile(@Res() res: ResponseServer) {
  const stream = createReadStream("./large-file.csv");
  
  res.setHeader("Content-Type", "text/csv");
  res.setHeader("Content-Disposition", "attachment; filename=data.csv");
  
  stream.pipe(res);
}
```

## Memory Management üß†

### Avoid Memory Leaks

```typescript
// ‚ùå Memory leak: unbounded cache
const cache = new Map();

@Get("/data")
getData(@Query("key") key: string, @Res() res: ResponseServer) {
  cache.set(key, data);  // Never cleaned up!
  res.json({ data });
}

// ‚úÖ Fixed: LRU cache with max size
class LRUCache<K, V> {
  private cache = new Map<K, V>();
  
  constructor(private maxSize: number) {}
  
  set(key: K, value: V) {
    if (this.cache.size >= this.maxSize) {
      const firstKey = this.cache.keys().next().value;
      this.cache.delete(firstKey);
    }
    this.cache.set(key, value);
  }
  
  get(key: K): V | undefined {
    return this.cache.get(key);
  }
}

const cache = new LRUCache(1000);  // Max 1000 entries
```

### Clean Up Resources

```typescript
@Post("/process")
async processFile(@Body() data: any, @Res() res: ResponseServer) {
  let tempFile: string | null = null;
  
  try {
    tempFile = await saveTempFile(data);
    const result = await processFile(tempFile);
    res.json({ result });
  } finally {
    // Always clean up
    if (tempFile) {
      await fs.unlink(tempFile);
    }
  }
}
```

## Benchmarking üìä

Use tools to measure performance:

```bash
# Autocannon (Node.js)
npm install -g autocannon
autocannon -c 100 -d 10 http://localhost:3000/api/users

# Apache Bench
ab -n 10000 -c 100 http://localhost:3000/api/users

# wrk
wrk -t12 -c400 -d30s http://localhost:3000/api/users
```

### Benchmark Example

```typescript
import autocannon from "autocannon";

async function benchmark() {
  const result = await autocannon({
    url: "http://localhost:3000",
    connections: 100,
    duration: 10,
    pipelining: 1,
    requests: [
      {
        method: "GET",
        path: "/api/users"
      }
    ]
  });
  
  console.log(`Requests/sec: ${result.requests.mean}`);
  console.log(`Latency (avg): ${result.latency.mean}ms`);
  console.log(`Throughput: ${result.throughput.mean} bytes/sec`);
}

benchmark();
```

## Monitoring üìà

```typescript
import { performance } from "perf_hooks";

function performanceMiddleware(req: RequestServer, res: ResponseServer, next: () => void) {
  const start = performance.now();
  
  res.on("finish", () => {
    const duration = performance.now() - start;
    
    console.log({
      method: req.method,
      url: req.url,
      statusCode: res.statusCode,
      duration: `${duration.toFixed(2)}ms`
    });
    
    // Alert if request takes too long
    if (duration > 1000) {
      console.warn(`‚ö†Ô∏è Slow request: ${req.method} ${req.url} took ${duration.toFixed(2)}ms`);
    }
  });
  
  next();
}

app.use(performanceMiddleware);
```

## Production Optimizations üöÄ

### Environment Variables

```typescript
const app = new AzuraClient({
  environment: "production",
  logging: {
    level: process.env.NODE_ENV === "production" ? "error" : "debug"
  }
});
```

### PM2 Configuration

```javascript
// ecosystem.config.js
module.exports = {
  apps: [{
    name: "azura-app",
    script: "./dist/server.js",
    instances: "max",  // Use all cores
    exec_mode: "cluster",
    env_production: {
      NODE_ENV: "production",
      PORT: 3000
    },
    max_memory_restart: "500M",
    error_file: "./logs/error.log",
    out_file: "./logs/out.log",
    log_date_format: "YYYY-MM-DD HH:mm:ss Z"
  }]
};
```

## Performance Checklist ‚úÖ

<Callout type="tip">
  ‚úÖ **Use cluster mode** to utilize all CPU cores
</Callout>

<Callout type="tip">
  ‚úÖ **Implement caching** for data that doesn't change frequently
</Callout>

<Callout type="tip">
  ‚úÖ **Optimize database queries** with proper indexes and JOINs
</Callout>

<Callout type="tip">
  ‚úÖ **Use connection pooling** for database
</Callout>

<Callout type="tip">
  ‚úÖ **Compress responses** with gzip
</Callout>

<Callout type="tip">
  ‚úÖ **Paginate results** to limit transferred data
</Callout>

<Callout type="tip">
  ‚úÖ **Use Promise.all** for parallel operations
</Callout>

<Callout type="tip">
  ‚úÖ **Monitor performance** with metrics and logs
</Callout>

<Callout type="warn">
  ‚ö†Ô∏è **Avoid event loop blocking** with heavy synchronous operations
</Callout>

<Callout type="warn">
  ‚ö†Ô∏è **Don't do N+1 queries** - always use JOINs or batch queries
</Callout>

## Performance Comparison üìä

**Single server vs Cluster (8 cores):**

```
Single Process:
  Requests/sec: 5,000
  Latency: 20ms

Cluster Mode (8 cores):
  Requests/sec: 35,000
  Latency: 3ms
  
Gain: 7x throughput, 6.6x latency
```

## Real-World Benchmarks üìä

### AzuraJS vs Popular Frameworks

Benchmark conditions: 8-core Intel i7, 16GB RAM, Node.js 20.x, wrk -t12 -c400 -d30s

| Framework | Req/sec | Latency (avg) | Latency (p99) | Memory (MB) |
|-----------|---------|---------------|---------------|-------------|
| **AzuraJS (cluster)** | **35,000** | **3ms** | **15ms** | **120** |
| Express.js | 12,000 | 8ms | 45ms | 180 |
| Fastify | 28,000 | 4ms | 18ms | 140 |
| Hono | 32,000 | 3.5ms | 16ms | 110 |
| Elysia (Bun) | 40,000 | 2.5ms | 12ms | 95 |

### Simple GET Endpoint

```typescript
// Benchmark code
@Get('/hello')
hello() {
  return { message: 'Hello World' };
}
```

**Results:**
```bash
$ wrk -t12 -c400 -d30s http://localhost:3000/hello

Running 30s test @ http://localhost:3000/hello
  12 threads and 400 connections
  
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     3.12ms    2.45ms   89.12ms   87.23%
    Req/Sec     2.91k   345.12    4.23k    73.45%
    
  1,047,234 requests in 30.01s, 215.43MB read
  
Requests/sec:  34,907.12
Transfer/sec:      7.18MB
```

### JSON API Endpoint

```typescript
@Get('/users/:id')
async getUser(@Param('id') id: string) {
  const user = await db.query('SELECT * FROM users WHERE id = $1', [id]);
  return { user };
}
```

**With Connection Pool:**
- Requests/sec: 15,000
- Latency (avg): 8ms
- Latency (p99): 35ms

**With Redis Cache:**
- Requests/sec: 28,000
- Latency (avg): 4ms
- Latency (p99): 18ms

## CPU Profiling üî¨

### Using Node.js Inspector

```bash
# Start with inspector
node --inspect server.js

# Or use --inspect-brk to break on start
node --inspect-brk server.js

# Then open Chrome DevTools
chrome://inspect
```

### Programmatic Profiling

```bash
npm install v8-profiler-next
```

```typescript
import profiler from 'v8-profiler-next';
import { writeFileSync } from 'fs';

// Start profiling
function startProfiling() {
  profiler.startProfiling('CPU Profile', true);
  log.info('CPU profiling started');
}

// Stop and save profile
function stopProfiling() {
  const profile = profiler.stopProfiling('CPU Profile');
  
  profile.export((error, result) => {
    writeFileSync('cpu-profile.cpuprofile', result);
    profile.delete();
    log.info('CPU profile saved to cpu-profile.cpuprofile');
  });
}

// Profile specific endpoint
@Get('/api/heavy')
async heavyOperation() {
  startProfiling();
  
  try {
    const result = await performHeavyOperation();
    return { result };
  } finally {
    stopProfiling();
  }
}
```

### Analyzing with Clinic.js

```bash
npm install -g clinic

# Diagnose performance issues
clinic doctor -- node server.js

# Profile CPU usage
clinic flame -- node server.js

# Analyze async operations
clinic bubbleprof -- node server.js
```

### Flame Graphs

```bash
# Install 0x
npm install -g 0x

# Generate flame graph
0x server.js

# Load test while profiling
# In another terminal:
autocannon -c 100 -d 30 http://localhost:3000

# Open flamegraph.html in browser
```

## Memory Management üß†

### Heap Snapshots

```typescript
import v8 from 'v8';
import { writeFileSync } from 'fs';

function takeHeapSnapshot(filename: string) {
  const snapshot = v8.writeHeapSnapshot(filename);
  log.info(`Heap snapshot saved to ${snapshot}`);
}

// Take snapshot on demand
@Get('/admin/heap-snapshot')
heapSnapshot() {
  const filename = `heap-${Date.now()}.heapsnapshot`;
  takeHeapSnapshot(filename);
  return { success: true, filename };
}

// Auto-snapshot on high memory
setInterval(() => {
  const usage = process.memoryUsage();
  const heapUsedMB = usage.heapUsed / 1024 / 1024;
  
  if (heapUsedMB > 500) {  // 500MB threshold
    log.warn(`High memory usage: ${heapUsedMB.toFixed(2)}MB`);
    takeHeapSnapshot(`auto-heap-${Date.now()}.heapsnapshot`);
  }
}, 60000);  // Check every minute
```

### Memory Leak Detection

```typescript
class MemoryMonitor {
  private samples: number[] = [];
  private readonly maxSamples = 10;
  
  check() {
    const usage = process.memoryUsage();
    const heapUsedMB = usage.heapUsed / 1024 / 1024;
    
    this.samples.push(heapUsedMB);
    
    if (this.samples.length > this.maxSamples) {
      this.samples.shift();
    }
    
    // Detect consistent growth
    if (this.samples.length === this.maxSamples) {
      const isGrowing = this.samples.every((val, i, arr) => 
        i === 0 || val > arr[i - 1]
      );
      
      if (isGrowing) {
        log.fatal('üö® Potential memory leak detected!', {
          samples: this.samples.map(s => `${s.toFixed(2)}MB`),
          current: `${heapUsedMB.toFixed(2)}MB`
        });
        
        // Take heap snapshot for analysis
        takeHeapSnapshot(`leak-${Date.now()}.heapsnapshot`);
      }
    }
  }
  
  report() {
    const usage = process.memoryUsage();
    
    return {
      heapUsed: `${(usage.heapUsed / 1024 / 1024).toFixed(2)} MB`,
      heapTotal: `${(usage.heapTotal / 1024 / 1024).toFixed(2)} MB`,
      external: `${(usage.external / 1024 / 1024).toFixed(2)} MB`,
      rss: `${(usage.rss / 1024 / 1024).toFixed(2)} MB`,
      arrayBuffers: `${(usage.arrayBuffers / 1024 / 1024).toFixed(2)} MB`
    };
  }
}

const memoryMonitor = new MemoryMonitor();

// Check every 5 minutes
setInterval(() => {
  memoryMonitor.check();
  log.info('Memory usage', memoryMonitor.report());
}, 300000);
```

### Garbage Collection Monitoring

```typescript
import { PerformanceObserver } from 'perf_hooks';

const obs = new PerformanceObserver((items) => {
  const entry = items.getEntries()[0];
  const duration = entry.duration;
  
  if (duration > 100) {  // GC pause > 100ms
    log.warn('Long GC pause detected', {
      duration: `${duration.toFixed(2)}ms`,
      kind: entry.detail?.kind,
      flags: entry.detail?.flags
    });
  }
});

obs.observe({ entryTypes: ['gc'], buffered: true });

// Manual garbage collection (requires --expose-gc flag)
if (global.gc) {
  @Get('/admin/gc')
  triggerGC() {
    const before = process.memoryUsage().heapUsed;
    global.gc();
    const after = process.memoryUsage().heapUsed;
    const freed = (before - after) / 1024 / 1024;
    
    return {
      freedMemory: `${freed.toFixed(2)}MB`,
      heapUsed: `${(after / 1024 / 1024).toFixed(2)}MB`
    };
  }
}
```

## Monitoring Tools üìà

### Built-in Metrics Endpoint

```typescript
import { performance } from 'perf_hooks';

class MetricsCollector {
  private metrics = {
    requests: {
      total: 0,
      success: 0,
      errors: 0,
      byMethod: {} as Record<string, number>,
      byStatus: {} as Record<number, number>
    },
    performance: {
      responseTimes: [] as number[],
      avgResponseTime: 0,
      p50: 0,
      p95: 0,
      p99: 0
    },
    system: {
      uptime: 0,
      memory: {},
      cpu: 0
    }
  };
  
  private startTime = Date.now();
  
  recordRequest(method: string, statusCode: number, duration: number) {
    this.metrics.requests.total++;
    
    if (statusCode < 400) {
      this.metrics.requests.success++;
    } else {
      this.metrics.requests.errors++;
    }
    
    this.metrics.requests.byMethod[method] = 
      (this.metrics.requests.byMethod[method] || 0) + 1;
    
    this.metrics.requests.byStatus[statusCode] = 
      (this.metrics.requests.byStatus[statusCode] || 0) + 1;
    
    this.metrics.performance.responseTimes.push(duration);
    
    // Keep only last 1000
    if (this.metrics.performance.responseTimes.length > 1000) {
      this.metrics.performance.responseTimes.shift();
    }
    
    this.updatePerformanceMetrics();
  }
  
  private updatePerformanceMetrics() {
    const times = [...this.metrics.performance.responseTimes].sort((a, b) => a - b);
    
    if (times.length === 0) return;
    
    this.metrics.performance.avgResponseTime = 
      times.reduce((a, b) => a + b, 0) / times.length;
    
    this.metrics.performance.p50 = times[Math.floor(times.length * 0.5)];
    this.metrics.performance.p95 = times[Math.floor(times.length * 0.95)];
    this.metrics.performance.p99 = times[Math.floor(times.length * 0.99)];
  }
  
  getMetrics() {
    const memory = process.memoryUsage();
    const uptime = Date.now() - this.startTime;
    
    return {
      requests: this.metrics.requests,
      performance: {
        avgResponseTime: `${this.metrics.performance.avgResponseTime.toFixed(2)}ms`,
        p50: `${this.metrics.performance.p50.toFixed(2)}ms`,
        p95: `${this.metrics.performance.p95.toFixed(2)}ms`,
        p99: `${this.metrics.performance.p99.toFixed(2)}ms`
      },
      system: {
        uptime: `${(uptime / 1000 / 60).toFixed(2)} minutes`,
        memory: {
          heapUsed: `${(memory.heapUsed / 1024 / 1024).toFixed(2)}MB`,
          heapTotal: `${(memory.heapTotal / 1024 / 1024).toFixed(2)}MB`,
          rss: `${(memory.rss / 1024 / 1024).toFixed(2)}MB`
        },
        nodeVersion: process.version,
        platform: process.platform,
        arch: process.arch
      }
    };
  }
}

const metrics = new MetricsCollector();

// Metrics middleware
app.use((req, res, next) => {
  const start = performance.now();
  
  res.on('finish', () => {
    const duration = performance.now() - start;
    metrics.recordRequest(req.method, res.statusCode, duration);
  });
  
  next();
});

// Metrics endpoint
@Get('/metrics')
getMetrics() {
  return metrics.getMetrics();
}

// Prometheus-compatible metrics
@Get('/metrics/prometheus')
getPrometheusMetrics() {
  const m = metrics.getMetrics();
  
  return `
# HELP http_requests_total Total HTTP requests
# TYPE http_requests_total counter
http_requests_total ${m.requests.total}

# HELP http_requests_success Successful HTTP requests
# TYPE http_requests_success counter
http_requests_success ${m.requests.success}

# HELP http_requests_errors Failed HTTP requests
# TYPE http_requests_errors counter
http_requests_errors ${m.requests.errors}

# HELP http_response_time_avg Average response time in ms
# TYPE http_response_time_avg gauge
http_response_time_avg ${parseFloat(m.performance.avgResponseTime)}
  `.trim();
}
```

### Integration with Monitoring Services

#### Prometheus

```bash
npm install prom-client
```

```typescript
import client from 'prom-client';

// Create metrics
const httpRequestDuration = new client.Histogram({
  name: 'http_request_duration_ms',
  help: 'Duration of HTTP requests in ms',
  labelNames: ['method', 'route', 'status_code']
});

const httpRequestTotal = new client.Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status_code']
});

// Middleware
app.use((req, res, next) => {
  const start = Date.now();
  
  res.on('finish', () => {
    const duration = Date.now() - start;
    
    httpRequestDuration.observe(
      { method: req.method, route: req.route?.path || req.url, status_code: res.statusCode },
      duration
    );
    
    httpRequestTotal.inc({
      method: req.method,
      route: req.route?.path || req.url,
      status_code: res.statusCode
    });
  });
  
  next();
});

// Metrics endpoint
@Get('/metrics')
async metrics() {
  return await client.register.metrics();
}
```

#### DataDog

```bash
npm install dd-trace
```

```typescript
import tracer from 'dd-trace';

tracer.init({
  service: 'azura-api',
  env: process.env.NODE_ENV,
  version: '1.0.0',
  logInjection: true
});

// Tracer automatically instruments HTTP requests
```

#### New Relic

```bash
npm install newrelic
```

```typescript
// Must be first import
import 'newrelic';

// Rest of your app
import { AzuraClient } from 'azurajs';
```

## Production Optimization Checklist ‚úÖ

### Application Level

- ‚úÖ Enable cluster mode for multi-core utilization
- ‚úÖ Implement caching strategy (Redis/in-memory)
- ‚úÖ Use connection pooling for databases
- ‚úÖ Enable compression for responses
- ‚úÖ Implement rate limiting
- ‚úÖ Use pagination for large datasets
- ‚úÖ Optimize database queries and indexes
- ‚úÖ Implement lazy loading for relationships
- ‚úÖ Use Promise.all() for parallel operations
- ‚úÖ Avoid blocking the event loop

### Infrastructure Level

- ‚úÖ Use load balancer (nginx, HAProxy)
- ‚úÖ Enable HTTP/2
- ‚úÖ Set up CDN for static assets
- ‚úÖ Configure reverse proxy caching
- ‚úÖ Use SSD storage
- ‚úÖ Optimize network configuration
- ‚úÖ Enable keep-alive connections
- ‚úÖ Configure OS-level optimizations (ulimit, TCP settings)

### Monitoring & Observability

- ‚úÖ Set up application monitoring (DataDog, New Relic)
- ‚úÖ Configure log aggregation (ELK, CloudWatch)
- ‚úÖ Implement distributed tracing
- ‚úÖ Set up alerting for critical metrics
- ‚úÖ Monitor error rates and types
- ‚úÖ Track performance metrics (response times, throughput)
- ‚úÖ Monitor resource usage (CPU, memory, disk)

### Security & Reliability

- ‚úÖ Implement health check endpoints
- ‚úÖ Configure graceful shutdown
- ‚úÖ Set up auto-restart on crashes (PM2)
- ‚úÖ Implement circuit breakers
- ‚úÖ Configure request timeouts
- ‚úÖ Sanitize user inputs
- ‚úÖ Use environment variables for secrets
- ‚úÖ Regular security updates

## Common Bottlenecks üöß

### 1. Blocking Event Loop

**Problem:**
```typescript
// ‚ùå Bad: Blocks event loop
app.get('/heavy', () => {
  let result = 0;
  for (let i = 0; i < 1000000000; i++) {
    result += i;
  }
  return { result };
});
```

**Solution:**
```typescript
// ‚úÖ Good: Use worker threads
import { Worker } from 'worker_threads';

app.get('/heavy', async () => {
  return new Promise((resolve, reject) => {
    const worker = new Worker('./worker.js');
    worker.on('message', resolve);
    worker.on('error', reject);
  });
});
```

### 2. N+1 Queries

**Problem:**
```typescript
// ‚ùå Bad: N+1 queries
const posts = await db.query('SELECT * FROM posts');
for (const post of posts) {
  post.author = await db.query('SELECT * FROM users WHERE id = $1', [post.author_id]);
}
```

**Solution:**
```typescript
// ‚úÖ Good: Single query with JOIN
const posts = await db.query(`
  SELECT posts.*, users.name as author_name
  FROM posts
  JOIN users ON posts.author_id = users.id
`);
```

### 3. Missing Indexes

**Problem:**
```sql
-- Slow query without index
SELECT * FROM users WHERE email = 'user@example.com';
```

**Solution:**
```sql
-- Create index
CREATE INDEX idx_users_email ON users(email);

-- Now fast!
SELECT * FROM users WHERE email = 'user@example.com';
```

### 4. Memory Leaks

**Problem:**
```typescript
// ‚ùå Bad: Unbounded cache
const cache = new Map();

app.get('/data', (req, res) => {
  cache.set(req.query.key, data);  // Never cleared!
});
```

**Solution:**
```typescript
// ‚úÖ Good: LRU cache with max size
import LRU from 'lru-cache';

const cache = new LRU({
  max: 1000,
  ttl: 1000 * 60 * 5  // 5 minutes
});
```

### 5. Sequential Operations

**Problem:**
```typescript
// ‚ùå Bad: Sequential (300ms total)
const users = await fetchUsers();  // 100ms
const posts = await fetchPosts();  // 100ms
const comments = await fetchComments();  // 100ms
```

**Solution:**
```typescript
// ‚úÖ Good: Parallel (100ms total)
const [users, posts, comments] = await Promise.all([
  fetchUsers(),
  fetchPosts(),
  fetchComments()
]);
```

## Troubleshooting Performance Issues üîç

### High CPU Usage

**Diagnosis:**
```bash
# Check CPU usage
top -p $(pgrep -f "node")

# Profile with 0x
0x server.js
```

**Common causes:**
- Heavy computation in request handlers
- Inefficient algorithms
- Regular expressions on large strings
- JSON parsing large payloads

**Solutions:**
- Move heavy computation to worker threads
- Optimize algorithms
- Use streaming for large payloads
- Implement caching

### High Memory Usage

**Diagnosis:**
```bash
# Check memory
node --expose-gc --max-old-space-size=4096 server.js

# Take heap snapshot
curl http://localhost:3000/admin/heap-snapshot
```

**Common causes:**
- Memory leaks (listeners, timers, caches)
- Large objects in memory
- Not releasing database connections
- Accumulating logs in memory

**Solutions:**
- Remove event listeners when done
- Clear timeouts/intervals
- Use connection pooling
- Implement log rotation
- Use WeakMap/WeakSet for caches

### Slow Response Times

**Diagnosis:**
```bash
# Load test
autocannon -c 100 -d 30 http://localhost:3000

# Check database query times
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';
```

**Common causes:**
- Slow database queries
- No caching
- N+1 queries
- Blocking operations

**Solutions:**
- Add database indexes
- Implement caching
- Optimize queries
- Use Promise.all() for parallel operations

### Low Throughput

**Diagnosis:**
```bash
# Benchmark
wrk -t12 -c400 -d30s http://localhost:3000
```

**Common causes:**
- Single process (not using cluster)
- Inefficient middleware
- Synchronous operations
- No connection pooling

**Solutions:**
- Enable cluster mode
- Optimize middleware order
- Use async/await properly
- Implement connection pooling

## Next Steps üìñ

<Cards>
  <Card title="Cluster Mode" href="/docs/en/cluster-mode" description="Scale across multiple cores" />
  <Card title="Caching" href="/docs/en/caching" description="Implement caching strategies" />
  <Card title="Database" href="/docs/en/database" description="Optimize database operations" />
  <Card title="Monitoring" href="/docs/en/monitoring" description="Monitor your application" />
</Cards>